# CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models

This repository contains the official PyTorch implementation of CineTrans, a novel framework for generating videos with controllable cinematic transitions via masked diffusion models.

[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/2508.11484) [![Project Page](https://img.shields.io/badge/Project-Website-blue)](https://uknowsth.github.io/CineTrans/)

## ğŸ¥ Demo
https://github.com/user-attachments/assets/6f112e2f-40e3-4347-bab8-3e08bfa9366c

## ğŸ“¥ Installation
1. Clone the Repository
```
git clone https://github.com/UknowSth/CineTrans.git
cd CineTrans
```
2. Set up Environment
```
conda create -n cinetrans python==3.11.9
conda activate cinetrans

pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

## ğŸ¤— Checkpoint  
### CineTrans-Unet
Download the required [model weights](https://huggingface.co/NumlockUknowSth/CineTrans-Unet/tree/main) and place them in the `ckpt/` directory.
```
ckpt/
â”‚â”€â”€ stable-diffusion-v1-4/
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ text_encoder/
â”‚   â”œâ”€â”€ tokenizer/  
â”‚   â”‚â”€â”€ unet/
â”‚   â””â”€â”€ vae_temporal_decoder/
â”‚â”€â”€ checkpoint.pt
â”‚â”€â”€ longclip-L.pt
```
### CineTrans-DiT
Download the weights of [Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/tree/main) and [lora weights](https://huggingface.co/NumlockUknowSth/CineTrans-DiT/tree/main). Place them as:
```
Wan2.1-T2V-1.3B/ # original weights
â”‚â”€â”€ google/
â”‚   â””â”€â”€ umt5-xxl/
â”‚â”€â”€ config.json
â”‚â”€â”€ diffusion_pytorch_model.safetensors
â”‚â”€â”€ models_t5_umt5-xxl-enc-bf16.pth
â”‚â”€â”€ Wan2.1_VAE.pth
ckpt/
â””â”€â”€ weights.pt # lora weights
```
## ğŸ–¥ï¸ Inference  
To run the inference, use the following command:
### CineTrans-Unet
```
python pipelines/sample.py --config configs/sample.yaml
```
Using a single A100 GPU, generating a single video takes approximately 40s. You can modify the relevant configurations and prompt in `configs/sample.yaml` to adjust the generation process.
### CineTrans-DiT
```
python generate.py
```
Using a single A100 GPU, generating a single video takes approximately 5min. You can modify the relevant configurations and prompt in `configs/t2v.yaml` to adjust the generation process.
## ğŸ–¼ï¸ Gallery  

| ![coffee_cup](https://github.com/user-attachments/assets/c89e9462-a77b-44eb-91b6-bfba4c4c1567) | ![white_flower](https://github.com/user-attachments/assets/f5dffe7a-69da-4cc9-ba53-3549f46df904) | ![snow](https://github.com/user-attachments/assets/85b4392d-f88c-496f-a08e-b9c5f6c8354c) |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Shot1:[0s,4s] Shot2:[4s,8s] | Shot1:[0s,4s] Shot2:[4s,8s] | Shot1:[0s,2.75s] Shot2:[2.75s,5.5s] Shot3:[5.5s,8s] |
| ![vintage](https://github.com/user-attachments/assets/96aa859f-e8cc-4efd-802d-417cfafcf764) | ![city_night](https://github.com/user-attachments/assets/d9e3644c-1bb3-43c6-a1dd-ea4f816c04f2) | ![sea](https://github.com/user-attachments/assets/2f80ddac-d339-4e1d-83f4-962489e2a464) |
| Shot1:[0s,2.5s] Shot2:[2.5s,5s] Shot3:[5s,8s] | Shot1:[0s,2.5s] Shot2:[2.5s,5s] Shot3:[5s,8s] | Shot1:[0s,3s] Shot2:[3s,6s] Shot3:[6s,8s] |

## ğŸ“‘ BiTeX  
If you find [CineTrans](https://github.com/Vchitect/CineTrans.git) useful for your research and applications, please cite using this BibTeX:
```
@misc{wu2025cinetranslearninggeneratevideos,
      title={CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models}, 
      author={Xiaoxue Wu and Bingjie Gao and Yu Qiao and Yaohui Wang and Xinyuan Chen},
      year={2025},
      eprint={2508.11484},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.11484}, 
}
```




